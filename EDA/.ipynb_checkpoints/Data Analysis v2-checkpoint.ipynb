{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Notebook V2  - Includes train + test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "from itertools import compress\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from scipy.stats import norm\n",
    "from scipy import spatial\n",
    "from scipy.stats import mode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from csv files. Data has been stored as dataframes in pickle and just needs to loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(\"../data/input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "#train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\sample_submission.csv\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prop_df = pd.read_csv(\"../data/input/properties_2016.csv\")\n",
    "#prop_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pickle the data frames\n",
    "#train_df.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df.pkl\")\n",
    "#prop_df.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\prop_df.pkl\")\n",
    "#test_df.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read dataframes from pickle\n",
    "train_df = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df.pkl\")\n",
    "prop_df = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\prop_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.merge(prop_df,on='parcelid',how='left')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = test_df.rename(columns={\"ParcelId\" : \"parcelid\"})\n",
    "test_df_all_data = test_df.merge(prop_df,on='parcelid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the handling of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Missing features\n",
    "missing_df = train_df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "missing_df = missing_df.ix[missing_df['missing_count']>0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "\n",
    "print missing_df['column_name'].count()\n",
    "\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "rects = ax.barh(ind, missing_df.missing_count.values, color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_title(\"Number of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29 features have more than 50% of the data missing. Refer to the handling of missing data sheet for details to fill the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Missing features\n",
    "missing_df_test = test_df_all_data.isnull().sum(axis=0).reset_index()\n",
    "missing_df_test.columns = ['column_name', 'missing_count']\n",
    "missing_df_test = missing_df_test.ix[missing_df_test['missing_count']>0]\n",
    "missing_df_test = missing_df_test.sort_values(by='missing_count')\n",
    "\n",
    "print missing_df_test['column_name'].count()\n",
    "\n",
    "ind = np.arange(missing_df_test.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "rects = ax.barh(ind, missing_df_test.missing_count.values, color='blue')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df_test.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Count of missing values\")\n",
    "ax.set_title(\"Number of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since test and train data have similar missing data ratio, we'll combine both into one dataframe to handle missing data and split later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df_v2 = test_df_all_data.drop(['201610','201611','201612','201710','201711','201712'],axis=1)\n",
    "test_df_v2.insert(1,\"logerror\",np.zeros(len(test_df_v2.index.values)))\n",
    "test_df_v2.insert(2,\"transactiondate\",np.zeros(len(test_df_v2.index.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split point for train_test: 90275 (last entry of train data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df,test_df_v2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_full.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3075492, 60)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with all NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_null_ixs = pd.isnull(train_df[\"longitude\"]).nonzero()[0]\n",
    "#train_df = train_df.drop(all_null_ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Zip code distribution - first look\n",
    "# NOte that zip codes are masked and do not denote the actual values\n",
    "\n",
    "df_x = train_df2[train_df2[\"regionidzip\"].values<100000]\n",
    "plt.figure(figsize=(12,8))\n",
    "g = sns.distplot(df_x.regionidzip.values,fit=norm, bins=100, kde=False )\n",
    "#g.set(xlim=(90000,110000))\n",
    "plt.xlabel('Zip', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance results from random forest model. The model was trained on AWS on 16 cores and took about 3 hours. Refer to Lakshay for the details of the model. The results have been imported in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# without transaction month\n",
    "imp_df = pd.read_csv(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\impv2.csv\")\n",
    "imp_df = imp_df.sort_values(by=['%IncMSE'])\n",
    "imp_df\n",
    "\n",
    "# For definitions of IncMSe and IncNOdePurity refer to the random forest R importance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.bar(imp_df[\"Name\"].values,imp_df[\"%IncMSE\"].values)\n",
    "a = imp_df[\"Name\"].values\n",
    "b = imp_df[\"%IncMSE\"].values\n",
    "names = np.ndarray.tolist(a)\n",
    "imp = np.ndarray.tolist(b)\n",
    "y_pos = np.arange(len(names))\n",
    "plt.figure(figsize=(16,20))\n",
    "plt.barh(y_pos,imp, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, names);\n",
    "plt.title(\"IncMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with transaction month\n",
    "imp_df2 = imp_df.sort_values(by=['IncNodePurity'])\n",
    "a2 = imp_df2[\"Name\"].values\n",
    "b2 = imp_df2[\"IncNodePurity\"].values\n",
    "names = np.ndarray.tolist(a2)\n",
    "imp = np.ndarray.tolist(b2)\n",
    "y_pos = np.arange(len(names))\n",
    "plt.figure(figsize=(16,20))\n",
    "plt.barh(y_pos,imp, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, names);\n",
    "plt.title(\"IncNodePurity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_df_nrmlzd = imp_df.copy()\n",
    "imp_df_nrmlzd[\"%IncMSE\"]= imp_df[\"%IncMSE\"]/imp_df[\"%IncMSE\"].max()\n",
    "imp_df_nrmlzd[\"IncNodePurity\"]= imp_df[\"IncNodePurity\"]/imp_df[\"IncNodePurity\"].max()\n",
    "imp_df_nrmlzd[\"avg\"] = (imp_df_nrmlzd[\"%IncMSE\"] + imp_df_nrmlzd[\"IncNodePurity\"])\n",
    "imp_df_nrmlzd.sort_values(by=[\"avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatter of two importance metrics\n",
    "x = imp_df_nrmlzd[\"%IncMSE\"].values\n",
    "y = imp_df_nrmlzd[\"IncNodePurity\"].values\n",
    "ann = imp_df_nrmlzd[\"Name\"].values\n",
    "plt.figure(figsize=(10,10))\n",
    "fig,ax = plt.subplots(figsize=(20,20))\n",
    "ax.scatter(x,y)\n",
    "ax.set_xlabel(\"IncMSE\")\n",
    "ax.set_ylabel(\"IncNodePurity\")\n",
    "plt.title('Feature Importance on both metrics %IncMSE and %IncNodePurity')\n",
    "for i,txt in enumerate(ann):\n",
    "    ax.annotate(txt,(x[i],y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above plots the feature importance with respect to IncMSe and IncNodePurity. Higher the metric more, important it is.\n",
    "\n",
    "This analysis also includes the importance of transaction month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_df_nrmlzd.to_csv(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\imp_plot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_4 = pd.DataFrame.from_csv(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\train_df_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seasonality effects\n",
    "a = train_df_4.groupby([\"transaction_month\"]).count()\n",
    "b1 = a[\"parcelid\"].values\n",
    "b2 = np.linspace(1,12,12)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(b2,b1,b2,b1,\"ro\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Num of Transactions\")\n",
    "plt.title(\"Seasonality\")\n",
    "plt.axvspan(10,12,alpha=0.3,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 4.1: Region City ID  - regionidcity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# region_city_id grouping\n",
    "x_tmp = train_df.groupby([\"regionidcity\"]).count()\n",
    "x_tmp['parcelid'].plot(kind='barh',figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tmp2 = train_df.groupby([\"regionidcity\"]).count()\n",
    "x_tmp2[\"parcelid\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep data for SVM classification\n",
    "train_df_region = train_df[['regionidcity','longitude','latitude']]\n",
    "missing_region_id = train_df_region.isnull().sum(axis=0).reset_index()\n",
    "tdf_region_no_null = train_df_region.dropna()\n",
    "tdf_region_all_null = train_df_region[~train_df_region.isin(tdf_region_no_null).all(1)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_train_plus_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training SVM model\n",
    "\n",
    "#from sklearn import svm\n",
    "\n",
    "data_tmp = tdf_region_no_null.as_matrix()\n",
    "y_ids = data_tmp[:,0]\n",
    "#creating labels from region ids\n",
    "region_ids = np.unique(y_ids)\n",
    "labels = np.linspace(1,len(region_ids),len(region_ids))\n",
    "y = np.zeros(len(y_ids))\n",
    "for i in range(len(y_ids)):\n",
    "    y[i] = labels[list(region_ids).index(y_ids[i])]\n",
    "    \n",
    "X = data_tmp[:,1:3]\n",
    "\n",
    "#clf = svm.SVC()\n",
    "#clf.fit(X,y)\n",
    "#pickle.dump(clf,open(\"region_id_svm.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predicting region id for the missing values\n",
    "\n",
    "#import pickle\n",
    "#pickle.dump(clf,open(\"region_id_svm.sav\",'wb'))\n",
    "#clf = pickle.load(open(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\region_id_svm.sav\",'rb'))\n",
    "data_to_predict = tdf_region_all_null.as_matrix()\n",
    "X_test = data_to_predict[:,1:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classification is USELESS!\n",
    "Let's plot and see what is going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = y#/177.\n",
    "cmap = sns.cubehelix_palette(n_colors=186,as_cmap=True)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "points = ax.scatter(X[:,0],X[:,1],s=2, c=z, cmap=cm)\n",
    "ax.scatter(X_test[:,0],X_test[:,1],s=5, c='k', marker='*',label='Missing')\n",
    "f.colorbar(points)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.title('Missing_region_ids')\n",
    "plt.legend(loc='upper left',markerscale = 5, prop={'size':20})\n",
    "\n",
    "\n",
    "# NOTE\n",
    "# Black points are the missing region ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Observation:\n",
    "\n",
    "SVM classification is pretty much useless as all the data points are being classified as just one region id - 20. Need to look more into the decision boundary of the classification in order to understand the classification.\n",
    "\n",
    "Another approach is to use mean of all the data points in the the region id and classify based on the minimum distance of the unknown point to the respective means of the regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Tree to be used for clustering applications\n",
    "tree = spatial.KDTree(zip(train_df_region[\"longitude\"].values,train_df_region[\"latitude\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class to classify fields based on clustering\n",
    "# Classify region ids using nearest region center method\n",
    "\n",
    "class cluster(object):\n",
    "    \n",
    "    def __init__(self,feature,tree=tree):\n",
    "        self.feature = feature\n",
    "\n",
    "    def classify(self,x,df,k):\n",
    "        [m,n] = tree.query(x,k=k)\n",
    "        near_ids = df[self.feature].iloc[n].values\n",
    "        near_ids = near_ids[~np.isnan(near_ids)]\n",
    "        near_ids.tolist()\n",
    "        a = [int(i) for i in near_ids]\n",
    "        counts = np.bincount(a)\n",
    "        try:\n",
    "            return np.argmax(counts)\n",
    "        except ValueError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "region_cluster = cluster(\"regionidcity\")\n",
    "\n",
    "tdf_region_null_filled = tdf_region_all_null.copy()\n",
    "\n",
    "iteration = [10,100,500,1000,1500]\n",
    "t = 0\n",
    "while train_df_region[\"regionidcity\"].isnull().sum(axis=0) > 0:\n",
    "    for j in list(tdf_region_null_filled.index.values):\n",
    "        x = np.array([train_df_region.loc[j][\"longitude\"],train_df_region.loc[j][\"latitude\"]])\n",
    "        train_df_region.loc[j][\"regionidcity\"] = region_cluster.classify(x,train_df_region,iteration[t])\n",
    "    \n",
    "    # Update dataframes \n",
    "    tdf_region_no_null = train_df_region.dropna()\n",
    "    tdf_region_null_filled = train_df_region[~train_df_region.isin(tdf_region_no_null).all(1)]\n",
    "    print train_df_region[\"regionidcity\"].isnull().sum(axis=0)     \n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot lat,long with complete data\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#tdf_region_complete = pd.concat([tdf_region_no_null,tdf_region_null_filled])\n",
    "tdf_region_complete = train_df_region\n",
    "data_comp = tdf_region_complete.as_matrix()\n",
    "X2 = data_comp[:,1:3]\n",
    "y2_ids = data_comp[:,0]\n",
    "\n",
    "#creating labels from region ids\n",
    "region_ids = np.unique(y2_ids)\n",
    "labels = np.linspace(1,len(region_ids),len(region_ids))\n",
    "y2 = np.zeros(len(y2_ids))\n",
    "for i in range(len(y2_ids)):\n",
    "    y2[i] = labels[list(region_ids).index(y2_ids[i])]\n",
    "\n",
    "z2 = y2\n",
    "cmap = sns.cubehelix_palette(n_colors=186,as_cmap=True)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "points = ax.scatter(X2[:,0],X2[:,1],s=2, c=z2, cmap=cm)\n",
    "f.colorbar(points)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.title('Filled_region_ids')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 4.2: Region Zip Id  - regionidzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# region_zip_id grouping\n",
    "x_tmp = train_df.groupby([\"regionidzip\"]).count()\n",
    "#x_tmp['parcelid'].plot(kind='barh',figsize=(10,30))\n",
    "#x_tmp['parcelid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prep data \n",
    "train_df_zip = train_df[['regionidzip','longitude','latitude']]\n",
    "missing_zip_id = train_df_zip.isnull().sum(axis=0).reset_index()\n",
    "tdf_zip_no_null = train_df_zip.dropna()\n",
    "tdf_zip_all_null = train_df_zip[~train_df_zip.isin(tdf_zip_no_null).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_zip_tmp = tdf_zip_no_null.as_matrix()\n",
    "y_zip_ids = data_zip_tmp[:,0]\n",
    "#creating labels from region ids\n",
    "zip_ids = np.unique(y_zip_ids)\n",
    "labels = np.linspace(1,len(zip_ids),len(zip_ids))\n",
    "y_zip = np.zeros(len(y_zip_ids))\n",
    "for i in range(len(y_zip_ids)):\n",
    "    y_zip[i] = labels[list(zip_ids).index(y_zip_ids[i])]\n",
    "    \n",
    "X_zip = data_zip_tmp[:,1:3]\n",
    "\n",
    "missing_data = tdf_zip_all_null.as_matrix()\n",
    "X_zip_missing = missing_data[:,1:3]\n",
    "\n",
    "z_zip = y_zip\n",
    "cmap = sns.cubehelix_palette(n_colors=388,as_cmap=True)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "points = ax.scatter(X_zip[:,0],X_zip[:,1],s=2, c=z_zip, cmap=cm)\n",
    "ax.scatter(X_zip_missing[:,0],X_zip_missing[:,1],s=40, c='k', marker='*',label='Missing')\n",
    "f.colorbar(points)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.title('Missing_zip_ids')\n",
    "plt.legend(loc='upper left',markerscale = 2, prop={'size':20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classify zip ids using nearest region center method\n",
    "\n",
    "zip_location_mean = np.zeros(shape=(len(zip_ids),2))\n",
    "\n",
    "for idx,val in enumerate(zip_ids):\n",
    "    df_tmp = train_df_zip[train_df_zip[\"regionidzip\"]==val]\n",
    "    inp_zip_location = df_tmp.as_matrix()[:,1:3]\n",
    "    zip_mean_tmp = inp_zip_location.mean(axis=0)\n",
    "    zip_location_mean[idx,:] = zip_mean_tmp\n",
    "    \n",
    "tdf_zip_null_filled = tdf_zip_all_null.copy()\n",
    "\n",
    "def classify_zip(x,zip_location_mean=zip_location_mean,zip_ids=zip_ids):\n",
    "    \n",
    "    dist = np.zeros(len(zip_ids))\n",
    "    for i in range(zip_location_mean.shape[0]):\n",
    "        dist[i] = np.sqrt(np.sum(np.square(x-zip_location_mean[i,:])))\n",
    "    \n",
    "    return  zip_ids[np.argmin(dist)]   \n",
    "  \n",
    "    \n",
    "for j in list(tdf_zip_null_filled.index.values):\n",
    "    x = np.array([tdf_zip_null_filled.ix[j][\"longitude\"],tdf_zip_null_filled.ix[j][\"latitude\"]])\n",
    "    tdf_zip_null_filled.ix[j][\"regionidzip\"] = classify_zip(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot lat,long with complete data\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "tdf_zip_complete = pd.concat([tdf_zip_no_null,tdf_zip_null_filled])\n",
    "data_zip_comp = tdf_zip_complete.as_matrix()\n",
    "X2_zip = data_zip_comp[:,1:3]\n",
    "y2_zip_ids = data_zip_comp[:,0]\n",
    "\n",
    "#creating labels from region ids\n",
    "zip_ids = np.unique(y2_zip_ids)\n",
    "labels = np.linspace(1,len(zip_ids),len(zip_ids))\n",
    "y2_zip = np.zeros(len(y2_zip_ids))\n",
    "for i in range(len(y2_zip_ids)):\n",
    "    y2_zip[i] = labels[list(zip_ids).index(y2_zip_ids[i])]\n",
    "\n",
    "z2_zip = y2_zip\n",
    "cmap = sns.cubehelix_palette(n_colors=388,as_cmap=True)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15,10))\n",
    "points = ax.scatter(X2_zip[:,0],X2_zip[:,1],s=2, c=z2_zip, cmap=cm)\n",
    "f.colorbar(points)\n",
    "plt.xlabel('longitude')\n",
    "plt.ylabel('latitude')\n",
    "plt.title('Filled_zip_ids')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save complete dataframes\n",
    "tdf_zip_complete.to_pickle('C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\tdf_zip_complete.pkl')\n",
    "tdf_region_complete.to_pickle('C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\notebooks\\\\models\\\\tdf_region_complete.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 4.3: Full Bathroom Count - fullbathcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate using bathroomcnt which includes half bathrooms as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = train_df[\"bathroomcnt\"].values - train_df[\"fullbathcnt\"].values\n",
    "diff_sorted = np.sort(diff)\n",
    "null_vals = diff_sorted[np.isnan(diff_sorted)]\n",
    "diff_no_null = diff_sorted[~np.isnan(diff_sorted)]\n",
    "x = np.arange(0, len(diff_no_null), 1)\n",
    "print(\"Number of NANs in fullbathcnt: %i\"%len(null_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x,diff_no_null)\n",
    "plt.xlabel(\"Number of parcels\")\n",
    "plt.ylabel(\"Diff\")\n",
    "plt.title(\"Difference between full_bath_cnt and total_bath_cnt\")\n",
    "diff_a = diff_no_null[x in diff_no_null[x]>=1.5]\n",
    "print(\"Number of values with diff greater than 0.5: %i\"%len(diff_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption:\n",
    "If the difference between total_bath_cnt and full_bath_cnt is 0.5 then it is highly likely that full_bath_count is floor(total_bath_count). For example, if total_bath_cnt is 2.5, full_bath_cnt should be 2.\n",
    "\n",
    "It is seen only 7 or 0.007% values in the difference are above 1.5. Hence the above assumption is reasonable given 99.993% values have the difference of 0.5 or less. The missing values for full_bath_cnt can be filled with floor(bathroomcnt)\n",
    "\n",
    "*total_bath_cnt is same as bathroomcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_fullbathcnt = train_df[[\"fullbathcnt\",\"bathroomcnt\"]]\n",
    "df_fullbathcnt_no_null = df_fullbathcnt.dropna()\n",
    "df_fullbathcnt_all_null = df_fullbathcnt[~df_fullbathcnt.isin(df_fullbathcnt_no_null).all(1)]\n",
    "fullbathcnt_null_ix = df_fullbathcnt_all_null.index.values\n",
    "df_fullbathcnt_all_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.ix[126]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Rows with fullbathcnt missing, have majority of fields missing too. Check the above the output. 1182 of 90k such rows can be ignored (atleast to begin with) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data_v2 : Removed missing values described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df_v2 = train_df.drop(train_df.index[fullbathcnt_null_ix])\n",
    "#train_df_v2.to_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_v2 = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_v2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_v2[\"basementsqft\"] = train_df_v2.basementsqft.replace(np.NaN,0)\n",
    "train_df_v2[\"taxdelinquencyflag\"] = train_df_v2.taxdelinquencyflag.replace(np.NaN,0)\n",
    "train_df_v2[\"taxdelinquencyyear\"] = train_df_v2.taxdelinquencyyear.replace(np.NaN,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"storytypeid\"].isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "x,y = np.mgrid[0:5,2:8]\n",
    "tree = spatial.KDTree(zip(x.ravel(),y.ravel()))\n",
    "pts = np.array([2.5,5])\n",
    "tree.query(pts,k=4)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.data[15,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df_region\n",
    "from scipy import spatial\n",
    "tree = spatial.KDTree(zip(train_df_region[\"longitude\"].values,train_df_region[\"latitude\"].values))\n",
    "[m,n] = tree.query([-118488536.,34280990.],k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf_region_all_null.loc[155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_partial_filled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx2 = pd.read_pickle(\"C:\\\\Users\\\\lakshay\\\\Documents\\\\zes\\\\zestims\\\\data\\\\input\\\\train_df_regionidneighborhood_completed_all_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064055, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
